{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f1f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd540859",
   "metadata": {},
   "source": [
    "**FOR MY COLLEAGUES**<br>\n",
    "Ignore the part with the dataframes, skip to the last two cells (the one with the class definition and the one after that) and run those. Be sure to have downloaded the file I put on drive and put it in a folder called \"files\". IMO, this is preferable to changing the path in the cell because if we were to do so we would end up uploading versions with different paths at every git push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ba85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs():\n",
    "    \n",
    "    if \"df_a2q\" not in os.listdir(\"files\") \\\n",
    "    or \"df_c2q\" not in os.listdir(\"files\") \\\n",
    "    or \"df_c2a\" not in os.listdir(\"files\"):\n",
    "    \n",
    "        file_names = [\"files/sx-stackoverflow-a2q.txt\",\n",
    "                      \"files/sx-stackoverflow-c2q.txt\",\n",
    "                      \"files/sx-stackoverflow-c2a.txt\"]\n",
    "\n",
    "        columns = [[\"user_answering\", \n",
    "                    \"user_questioning\", \n",
    "                    \"time_u\"],\n",
    "                   [\"user_commenting\", \n",
    "                    \"user_questioning\", \n",
    "                    \"time_u\"],\n",
    "                   [\"user_commenting\", \n",
    "                    \"user_answering\", \n",
    "                    \"time_u\"]]\n",
    "\n",
    "        df_a2q_raw = pd.read_csv(file_names[0],\n",
    "                                 delim_whitespace=True,\n",
    "                                 names=columns[0])\n",
    "\n",
    "        df_c2q_raw = pd.read_csv(file_names[1],\n",
    "                                 delim_whitespace=True,\n",
    "                                 names=columns[1])\n",
    "\n",
    "        df_c2a_raw = pd.read_csv(file_names[2],\n",
    "                                 delim_whitespace=True,\n",
    "                                 names=columns[2])\n",
    "        \n",
    "        \n",
    "        dfs_raw = [df_a2q_raw, df_c2q_raw, df_c2a_raw]\n",
    "\n",
    "        print(df_a2q_raw.shape)\n",
    "\n",
    "        # add a column for a standard visualization of dates\n",
    "        for df in dfs_raw:\n",
    "            df[\"time_h\"] = pd.to_datetime(df[\"time_u\"], unit=\"s\")\n",
    "\n",
    "        names = [\"a2q\", \"c2q\", \"c2a\"]\n",
    "        for i, df in enumerate(dfs_raw):\n",
    "            most_recent = df[\"time_h\"].max()\n",
    "            name = names[i]\n",
    "            print(f\"Most recent date for dataset {name}: {most_recent}\")\n",
    "\n",
    "        threshold = datetime(year=2014, month=1, day=1, hour=0, minute=0, second=0)\n",
    "        print(f\"Date threshold selected: {threshold}\")\n",
    "\n",
    "        df_a2q = df_a2q_raw[df_a2q_raw[\"time_h\"] > threshold].copy()\n",
    "        df_c2q = df_c2q_raw[df_c2q_raw[\"time_h\"] > threshold].copy()\n",
    "        df_c2a = df_c2a_raw[df_c2a_raw[\"time_h\"] > threshold].copy()\n",
    "\n",
    "        dfs = [df_a2q, df_c2q, df_c2a]\n",
    "        for df in dfs:\n",
    "            min_el = df[\"time_u\"].min()\n",
    "            max_el = df[\"time_u\"].max()\n",
    "            \n",
    "            # variation on min max scaler so to have values closer to the max -> to 0 and values\n",
    "            # closer to the min -> to 1.\n",
    "            df[\"weight\"] = (max_el - df[\"time_u\"]) / (max_el - min_el)\n",
    "            \n",
    "            df.drop(\"time_u\", inplace=True, axis=1)\n",
    "\n",
    "        with open(\"files/df_a2q\", \"wb\") as file:\n",
    "            pickle.dump(df_a2q, file)\n",
    "\n",
    "        with open(\"files/df_c2q\", \"wb\") as file:\n",
    "            pickle.dump(df_c2q, file)\n",
    "\n",
    "        with open(\"files/df_c2a\", \"wb\") as file:\n",
    "            pickle.dump(df_c2a, file)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        with open(\"files/df_a2q\", \"rb\") as file:\n",
    "            df_a2q = pickle.load(file)\n",
    "        \n",
    "        with open(\"files/df_c2q\", \"rb\") as file:\n",
    "            df_c2q = pickle.load(file)\n",
    "            \n",
    "        with open(\"files/df_c2a\", \"rb\") as file:\n",
    "            df_c2a = pickle.load(file)\n",
    "    \n",
    "    return df_a2q, df_c2q, df_c2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"a2q\", \"c2q\", \"c2a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2q, df_c2q, df_c2a = load_dfs()\n",
    "print(df_a2q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbe47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22337458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c2q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8806680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c2a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_a2q,\n",
    "       df_c2q,\n",
    "       df_c2a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_wrapper(func):\n",
    "    \"\"\"\n",
    "    Decorator to time functions, probably we won't need it in the final version, but for now I'm leaving it \n",
    "    in case you need it too. Just put @time_wrapper above any function you want to time and it will print\n",
    "    the time in seconds it took to run.\n",
    "    \"\"\"\n",
    "    def wrapped(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        r = func(*args, **kwargs)\n",
    "        print(f\"Time elapsed: {time.time()-start}\")\n",
    "        return r\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def visualize_network(graph, edges_to_highlight=[], highlight_nodes=True):\n",
    "    \n",
    "    nx_g = nx.MultiGraph(graph)\n",
    "    black_edges = [edge for edge in nx_g.edges() if edge not in edges_to_highlight]\n",
    "    nodes_to_highlight=[]\n",
    "    if highlight_nodes:\n",
    "        for edge in edges_to_highlight:\n",
    "            for node in edge:\n",
    "                if node not in nodes_to_highlight:\n",
    "                    nodes_to_highlight.append(node)\n",
    "        blank_nodes = [node for node in nx_g.nodes() if node not in nodes_to_highlight]\n",
    "    \n",
    "    plt.figure(figsize=(30, 20))\n",
    "    pos = nx.spring_layout(nx_g)\n",
    "    \n",
    "    nx.draw_networkx_nodes(nx_g, pos, cmap=plt.get_cmap('jet'), node_size = 100, nodelist=nodes_to_highlight, node_color=\"r\")\n",
    "    nx.draw_networkx_nodes(nx_g, pos, cmap=plt.get_cmap('jet'), node_size = 100, nodelist=blank_nodes, alpha=0.2)\n",
    "    nx.draw_networkx_edges(nx_g, pos, edgelist=edges_to_highlight, edge_color='r', arrows=True)\n",
    "    nx.draw_networkx_edges(nx_g, pos, edgelist=black_edges, alpha=0.2, arrows=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "                \n",
    "    \n",
    "    @time_wrapper\n",
    "    def create_graph(self, sources, names):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sources: dataframes from which to create the graph\n",
    "            names: a list of string containing the names of the graph used in order to label the edges accordingly\n",
    "        \n",
    "        Summary:\n",
    "            Each node in the graph acts as a key in the dictionary self.graph. Each entry in the dictionary is itself\n",
    "            another dictionary, indexed by that node's neighbors and pointing to a list with the details of that edge,\n",
    "            specifically [type_of_interaction, weight, date_human_format].\n",
    "            So to make things clearer:\n",
    "            \n",
    "            self.graph -> Our entire graph, indexed by node\n",
    "            self.graph[node \"A\"] -> dictionary indexed by the neighbors of \"A\"\n",
    "            self.graph[node \"A\"][node \"B\"] -> dictionary indexed by the time the interation took place\n",
    "            self.graph[node \"A\"][node \"B\"][time \"x\"] -> details (specifically type of interaction and edge weight) of the\n",
    "                                                        of the interaction between user A and B that took place at time x            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.graph = {}\n",
    "    \n",
    "        for i_source, source in enumerate(sources):\n",
    "            nodes_column = source.iloc[:, 0]\n",
    "            nodes = nodes_column.unique()\n",
    "            type_of_interaction = names[i_source]\n",
    "            \n",
    "            for node in tqdm(nodes):\n",
    "                if node not in self.graph.keys():\n",
    "                    self.graph[node] = {}\n",
    "                node_subdf = source[nodes_column == node]\n",
    "                neighbors = node_subdf.iloc[:, 1].values\n",
    "                times_h = node_subdf.iloc[:, 2].values\n",
    "                weights = node_subdf.iloc[:, 3].values\n",
    "                for i, neighbor in enumerate(neighbors):\n",
    "                    if neighbor not in self.graph.keys(): # here we want to make sure that if we're working on a subset\n",
    "                                                          # of data we still obtain a coherent graph. Otherwise we may\n",
    "                                                          # end up with neighbors that don't exist as actual nodes\n",
    "                                                          # in our graph.\n",
    "                        self.graph[neighbor] = {}\n",
    "                    if neighbor not in self.graph[node].keys():\n",
    "                        self.graph[node][neighbor] = {}\n",
    "                        \n",
    "                    self.graph[node][neighbor][times_h[i]] = {\"weight\": weights[i], \n",
    "                                                              \"type_of_interaction\": type_of_interaction}\n",
    "        \n",
    "    \n",
    "    @time_wrapper\n",
    "    def save_graph(self, path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: name of file where we want the file stored\n",
    "        Summary:\n",
    "            Saves graph as a binary file.\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(path, \"wb\") as file:\n",
    "            pickle.dump(self.graph, file)\n",
    "\n",
    "    \n",
    "    @time_wrapper\n",
    "    def load_graph(self, path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: name of file to load the graph from.\n",
    "        Summary:\n",
    "            Load graph from binary file.\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(path, \"rb\") as file:\n",
    "            self.graph = pickle.load(file)\n",
    "    \n",
    "    \n",
    "    def get_random_subgraph(self, n_nodes=10000):\n",
    "        np.random.seed(42)\n",
    "        bad_starting_node = True\n",
    "        while bad_starting_node:\n",
    "            random_node = np.random.choice(list(self.graph.keys()))\n",
    "            if len(self.graph[random_node].keys()) > 10:\n",
    "                bad_starting_node = False\n",
    "        print(random_node)\n",
    "        print(len(self.graph[random_node].keys()))\n",
    "        subgraph = {random_node: {}}\n",
    "        to_check = [random_node]\n",
    "        i = 0\n",
    "        \n",
    "        while len(subgraph.keys()) < n_nodes:\n",
    "            node = to_check[i]\n",
    "            if node not in subgraph.keys() or len(subgraph[node].keys()) < 1:\n",
    "                for neighbor in self.graph[node].keys():\n",
    "                    if neighbor not in to_check:\n",
    "                        to_check.append(neighbor)\n",
    "                    if neighbor not in subgraph.keys():\n",
    "                        subgraph[neighbor] = {}\n",
    "                    subgraph[node][neighbor] = self.graph[node][neighbor]\n",
    "                    if len(subgraph.keys()) >= n_nodes:\n",
    "                        break\n",
    "            i += 1\n",
    "        \n",
    "        return subgraph\n",
    "    \n",
    "    \n",
    "    def dijkstra(self, start_node, end_node, gr, visualization=True):\n",
    "        unvisited = []\n",
    "        for key in gr.keys():\n",
    "            if key not in unvisited:\n",
    "                unvisited.append(key)\n",
    "        visited = []\n",
    "        \n",
    "        min_distances = {key: (np.inf, None) for key in gr.keys()}\n",
    "        min_distances[start_node] = (0, None)\n",
    "        \n",
    "        current_node = start_node\n",
    "        \n",
    "        while current_node != end_node:\n",
    "            visited.append(current_node)\n",
    "            unvisited.remove(current_node)\n",
    "            for neighbor in gr[current_node].keys():\n",
    "                weights = [gr[current_node][neighbor][time_key][0] for time_key in gr[current_node][neighbor].keys()]\n",
    "                weight = np.min(weights)\n",
    "                tot = min_distances[current_node][0] + weight\n",
    "                if tot < min_distances[neighbor][0]:\n",
    "                    min_distances[neighbor] = (tot, current_node)\n",
    "            \n",
    "            min_unvisited = np.inf\n",
    "            found_something = False\n",
    "            for candidate in unvisited:\n",
    "                if min_distances[candidate][0] < min_unvisited:\n",
    "                    min_unvisited = min_distances[candidate][0]\n",
    "                    current_node = candidate\n",
    "                    found_something = True\n",
    "            \n",
    "            if not found_something:\n",
    "                break\n",
    "        \n",
    "        node_a = None\n",
    "        node_b = end_node\n",
    "        \n",
    "        path = []\n",
    "        while node_a != start_node:\n",
    "            node_a = min_distances[node_b][1]\n",
    "            if node_a == None:\n",
    "                break\n",
    "            path.append((node_a, node_b))\n",
    "            node_b = node_a\n",
    "        \n",
    "        if len(path) <= 0:\n",
    "            print(\"No path found.\")\n",
    "        \n",
    "        else:\n",
    "            path = path[::-1]\n",
    "            if visualization:\n",
    "                visualize_network(gr, path)\n",
    "        \n",
    "        return path, min_distances\n",
    "    \n",
    "    \n",
    "    @time_wrapper\n",
    "    def get_neighbors(self, node, interaction=\"all\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node: the node we want the neighbors of.\n",
    "            interaction: the label of the edges we're interested in, default=all, meaning we will obtain all\n",
    "                         neighbors of that node, regardless of the type of interactions.\n",
    "        Summary:\n",
    "            Obtains all the neighbors of a specific node.\n",
    "        \"\"\"\n",
    "        \n",
    "        if interaction == \"all\":\n",
    "            neighbors = list(self.graph[node].keys())\n",
    "        else:\n",
    "            neighbors = []\n",
    "            for neighbor in self.graph[node].keys():\n",
    "                if self.graph[node][neighbor][0] == interaction:\n",
    "                    neighbors.append(neighbor)\n",
    "        \n",
    "        return neighbors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running this cell, make sure to have the pickle file in a \"files\" folder, or change the path of the if \n",
    "# condition accordingly. This will take some time: on my pc it takes approximately 7 minutes to load the graph.\n",
    "# The timing decorator will tell you how much it took, so you can plan accordingly.\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "if \"merged_graph.p\" in os.listdir(\"files\"):\n",
    "    g.load_graph(\"files/merged_graph.p\")\n",
    "else:\n",
    "    g.create_graph(dfs, names=names)\n",
    "    g.save_graph(\"files/merged_graph.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35826540",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.create_graph(dfs, names)\n",
    "g.save_graph(\"files/merged_graph_2.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "subg = g.get_random_subgraph(n_nodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32588183",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(list(subg.keys()), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1491895\n",
    "for key in subg[3043801].keys():\n",
    "    if len(list(subg[key].keys())) > 3:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6749ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "subg[596952].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.dijkstra(start_node=3043801,\n",
    "           end_node=3128099,\n",
    "           gr=subg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c507103",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_network(subg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d889d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This for lightweight graph\n",
    "\n",
    "g = Graph()\n",
    "g.load_graph(\"files/merged_graph_lw.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_network(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f5ef3",
   "metadata": {},
   "source": [
    "## Functionality 4 (Work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11841178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_for_time_interval(start, end, dfs, names):\n",
    "    \n",
    "    start = datetime.strptime(start, \"%d %B %Y %H:%M:%S\")\n",
    "    end = datetime.strptime(end, \"%d %B %Y %H:%M:%S\")\n",
    "    print(f\"start: {start}\")\n",
    "    print(f\"end: {end}\")\n",
    "    \n",
    "    subdfs = []\n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        sub_df = df[df[\"time_h\"] >= start]\n",
    "        sub_df = sub_df[sub_df[\"time_h\"] < end]\n",
    "        sub_df.columns = [\"user_a\", \"user_b\", \"time_h\", \"weight\"]\n",
    "        sub_df[\"type\"] = names[i]\n",
    "        subdfs.append(sub_df)\n",
    "    \n",
    "    return pd.concat(subdfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa196dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_1 = \"10 June 2015 15:00:00\"\n",
    "end_1 = \"10 June 2015 19:00:00\"\n",
    "\n",
    "df_g1 = get_df_for_time_interval(start_1, end_1, dfs, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d95e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfbb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgraph_from_df(df):\n",
    "    subg = {}\n",
    "    nodes = np.unique(df[\"user_a\"].values)\n",
    "    for node in nodes:\n",
    "        \n",
    "        if node not in subg.keys():\n",
    "            subg[node] = {}\n",
    "        \n",
    "        subdf = df[df[\"user_a\"]==node]\n",
    "        neighs = subdf[\"user_b\"].values\n",
    "        times = subdf[\"time_h\"].values\n",
    "        weights = subdf[\"weight\"].values\n",
    "        type_of_interaction = subdf[\"type\"].values\n",
    "        \n",
    "        for i, neigh in enumerate(neighs):\n",
    "            if neigh not in subg.keys():\n",
    "                subg[neigh] = {}\n",
    "            if neigh not in subg[node].keys():\n",
    "                subg[node][neigh] = {}\n",
    "            subg[node][neigh][times[i]] = {\"weight\": weights[i], \n",
    "                                           \"type_of_interaction\": type_of_interaction[i]}\n",
    "        \n",
    "    return subg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abccdccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disconnect_nodes(subg, node_1, node_2):\n",
    "    \n",
    "    removed_edges = []\n",
    "    shortest_path = [node_1]\n",
    "    \n",
    "    while len(shortest_path) > 0:\n",
    "        shortest_path, distances_m = g.dijkstra(start_node=node_1, end_node=node_2, gr=subg, visualization=False)\n",
    "        cheapest_value = np.inf\n",
    "        candidate = (None, None)\n",
    "\n",
    "        while node_b != node_1:\n",
    "            value, node_a = distances_m[node_b]\n",
    "            if value < cheapest_value:\n",
    "                candidate_edge = (key, node_b)\n",
    "                \n",
    "        removed_edges.append(candidate_edge)\n",
    "        subg[candidate_edge[0]].pop(candidate_edge[1])\n",
    "    \n",
    "    return removed_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f1455",
   "metadata": {},
   "outputs": [],
   "source": [
    "subg_1 = get_subgraph_from_df(df=df_g1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_network(subg_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91707907",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(list(subg_1.keys()), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "disconnect_nodes(subg_1, 2595183, 4930287)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_g = nx.MultiGraph(subg_1)\n",
    "nx_g.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_network(subg_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c81e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d064fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subg_1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d3013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fds]",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
